{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data (and select part of it only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_data_fraction = 20000\n",
    "x_train = x_train[0:train_data_fraction]\n",
    "y_train = y_train[0:train_data_fraction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Size and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT DATA\n",
    "class_names = ['Zero','One', 'Two', 'Three', 'Four', 'Five','Six', 'Seven', 'Eight', 'Nine']\n",
    "N_image_channels = 1\n",
    "\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_class = len(class_names)\n",
    "image_shape = x_train.shape[1:3]\n",
    "input_range = np.amax(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot images from set (random or in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(x_train, y_train, figures_to_plot=10, pick_random=True):\n",
    "    n_rows = np.ceil(figures_to_plot/10)\n",
    "    plot = plt.figure(figsize=[20,2*n_rows])\n",
    "    for i in range(figures_to_plot):\n",
    "        if pick_random: \n",
    "            pic_n = random.randint(0,len(x_train))\n",
    "        else: pic_n = i\n",
    "        plt.subplot(n_rows,10,i+1)\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        plt.imshow(x_train[pic_n], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[y_train[pic_n]])\n",
    "    plt.show()\n",
    "\n",
    "plot_images(x_train, y_train, 10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Proces Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = x_train/input_range\n",
    "test_images = x_test/input_range\n",
    "\n",
    "train_images[train_images >= 0.5] = 1.0\n",
    "train_images[train_images < 0.5] = 0.0\n",
    "test_images[test_images >= 0.5] = 1.0\n",
    "test_images[test_images < 0.5] = 0.0\n",
    "\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "\n",
    "plot_images(train_images, train_labels, 10, False)\n",
    "\n",
    "train_images = train_images.reshape(N_train, image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
    "test_images = test_images.reshape(N_test, image_shape[0], image_shape[1], N_image_channels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "shuffle_size_train = 20000\n",
    "shuffle_size_test = 10000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images)).shuffle(shuffle_size_train).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images)).shuffle(shuffle_size_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Variational Autoencoder (VAE) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, z_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        self.encoder_nn = tf.keras.models.Sequential([ \n",
    "                          tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "                          tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                          tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                          tf.keras.layers.Flatten(),\n",
    "                          # No activation\n",
    "                          tf.keras.layers.Dense(z_size*2)\n",
    "                          ])\n",
    "\n",
    "        self.decoder_nn = tf.keras.models.Sequential([\n",
    "                          tf.keras.layers.InputLayer(input_shape=(z_size,)),\n",
    "                          tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                          tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                          tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'),\n",
    "                          tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'),\n",
    "                          # No activation\n",
    "                          tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\")\n",
    "                          ])\n",
    "    @tf.function\n",
    "    def encode(self, x):\n",
    "        encoder_nn_output = self.encoder_nn(x)\n",
    "        z_mean, z_logvar = tf.split(encoder_nn_output, num_or_size_splits=2, axis=1)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        eps = tf.random.normal(shape=z_mean.shape)\n",
    "        return eps * tf.exp(z_logvar * 0.5) + z_mean\n",
    "    \n",
    "    def decode(self, z):\n",
    "        pixel_output = self.decoder_nn(z)\n",
    "        return pixel_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(model, x):\n",
    "    z_mean, z_logvar = model.encode(x)\n",
    "    z = model.reparameterize(z_mean, z_logvar)\n",
    "    \n",
    "    pixel_output = model.decode(z)\n",
    "    #pixel_prob = tf.math.sigmoid(pixel_output)\n",
    "\n",
    "    logpx_z_pixels = -tf.nn.sigmoid_cross_entropy_with_logits(logits=pixel_output, labels=x)\n",
    "    #logpx_z_pixels = tf.math.log(pixel_prob + 1e-10)*x + tf.math.log(1-pixel_prob + 1e-10)*(1-x)\n",
    "    logpx_z_images = tf.reduce_sum(logpx_z_pixels, axis=[1, 2, 3])\n",
    "    logpx_z = tf.reduce_mean (logpx_z_images)\n",
    "    \n",
    "    logpz_z_parameters = -0.5 * (z ** 2.0 + np.log(2.0 * np.pi))\n",
    "    logpz_z_vector = tf.reduce_sum(logpz_z_parameters, axis=1)\n",
    "    logpz = tf.reduce_mean (logpz_z_vector)\n",
    "    \n",
    "    logqz_x_parameters = -0.5 * ((z - z_mean) ** 2.0 * tf.exp(-z_logvar) + z_logvar + np.log(2.0 * np.pi))\n",
    "    logqz_x_vectors = tf.reduce_sum(logqz_x_parameters, axis=1)\n",
    "    logqz_x = tf.reduce_mean (logqz_x_vectors)\n",
    "    \n",
    "    #kl_parameters = 0.5 * (1 + z_logvar - (z_mean ** 2) - tf.exp(-z_logvar))\n",
    "    #kl_vectors = tf.reduce_sum(kl_parameters, axis=1)\n",
    "    #kl = tf.reduce_mean(kl_vectors)\n",
    "    \n",
    "    return -(logpx_z + logpz - logqz_x) #### why negative?? ####\n",
    "    #return -(logpx_z + kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4) #### why doesn't it work for 1e-3?? ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the loss function gradients and input these to the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "z_size = 50\n",
    "model = CVAE(z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_random = tf.random.normal(shape=[16, z_size])\n",
    "\n",
    "def generate_and_save_images(model, z_random): \n",
    "    generated_output = model.decode(z_random)\n",
    "    generated_prob = tf.math.sigmoid(generated_output)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(generated_prob.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(generated_prob[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_images(model, z_random)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for i, train_x in enumerate(train_dataset): \n",
    "        sys.stdout.write('\\r'+'Epoch {} progress (%): {}'.format(epoch,100*(i+1)/(N_train/batch_size)))\n",
    "        sys.stdout.flush()\n",
    "        compute_apply_gradients(model, train_x, optimizer)\n",
    "    end_time = time.time()\n",
    "\n",
    "    average_loss = 0\n",
    "    for i, test_x in enumerate(test_dataset):\n",
    "        batch_loss = compute_loss(model, test_x)\n",
    "        average_loss = (average_loss*i + batch_loss)/(i+1)\n",
    "    print('\\nTest set ELBO: {}; epoch running time: {}'.format(average_loss, end_time - start_time))\n",
    "    \n",
    "    generate_and_save_images(model, z_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
