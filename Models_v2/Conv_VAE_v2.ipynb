{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CVAE_MNIST_Numbers.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzosintini/4YP/blob/master/Models_v2/Conv_VAE_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8VH0HaF0eaI"
      },
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "887mv3aj0eaL",
        "outputId": "5406db89-6638-441f-f62b-88520cd48476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve\n",
        "tf.autograph.set_verbosity(0)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sUEIvx3I0eaT"
      },
      "source": [
        "#### Import Data and plot it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkekihPQ0eaU",
        "colab": {}
      },
      "source": [
        "def plot_images(x_train, y_train, figures_to_plot, pick_random=False, include_labels=True):\n",
        "    n_rows = np.ceil((figures_to_plot[1])/10)\n",
        "    plot = plt.figure(figsize=[20,2*n_rows])\n",
        "    for i in range(figures_to_plot[0],figures_to_plot[1]):\n",
        "        if pick_random: \n",
        "            pic_n = random.randint(0,len(x_train))\n",
        "        else: pic_n = i\n",
        "        plt.subplot(n_rows,10,i+1)\n",
        "        plt.xticks([]); plt.yticks([])\n",
        "        plt.imshow(x_train[pic_n], cmap=plt.cm.binary)\n",
        "        if include_labels:\n",
        "            plt.xlabel(y_train[pic_n])\n",
        "    plt.show()\n",
        "\n",
        "#plot_images(x_train, y_train, [0,10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lv1qfPcr0eaZ"
      },
      "source": [
        "#### Select data, pre-process it and create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SXK5XnqV0eab",
        "colab": {}
      },
      "source": [
        "def pick_class(x, y, class_n):\n",
        "    pics = (y == class_n[0])\n",
        "    for i in range(len(class_n)-1):\n",
        "      pics = pics + (y == class_n[i+1])\n",
        "    new_x = x[pics]\n",
        "    new_y = y[pics]\n",
        "    return new_x, new_y\n",
        "\n",
        "def set_pixels_binary(images):\n",
        "    images = images/input_range\n",
        "    #images[images >= 0.5] = 1.0\n",
        "    #images[images < 0.5] = 0.0\n",
        "    return images\n",
        "\n",
        "def make_categorical(y):\n",
        "    y_cat = tf.keras.utils.to_categorical(y)\n",
        "    return y_cat\n",
        "\n",
        "def cut_data(data, data_number):\n",
        "    data = data[0:data_number]\n",
        "    return data\n",
        "\n",
        "def setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[0,1,2,3,4,5,6,7,8,9], data_number = -1):\n",
        "  x_train, y_train = pick_class(x_train, y_train, chosen_classes)\n",
        "  x_test, y_test = pick_class(x_test, y_test, chosen_classes)\n",
        "  \n",
        "  x_train = cut_data(x_train, data_number)\n",
        "  y_train = cut_data(y_train, data_number)\n",
        "\n",
        "  train_images = set_pixels_binary(x_train)\n",
        "  test_images = set_pixels_binary(x_test)\n",
        "\n",
        "  train_images = train_images.reshape(len(y_train), image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
        "  test_images = test_images.reshape(len(y_test), image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
        "\n",
        "  batch_size = 100\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((train_images)).shuffle(len(y_train)).batch(batch_size)\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((test_images)).shuffle(len(y_test)).batch(batch_size)\n",
        "  return x_train, y_train, x_test, y_test, train_dataset, test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RXAUM1Hh0ea0"
      },
      "source": [
        "#### Create Variational Autoencoder (VAE) Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oiLqehle0ea2",
        "colab": {}
      },
      "source": [
        "class CVAE(tf.keras.Model):\n",
        "    def __init__(self, z_size):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.z_size = z_size\n",
        "        self.encoder_nn = tf.keras.models.Sequential([ \n",
        "                          tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "                          tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                          tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(z_size*2)\n",
        "                          ])\n",
        "\n",
        "        self.decoder_nn = tf.keras.models.Sequential([\n",
        "                          tf.keras.layers.Dense(units=24*24*64, activation='relu', input_shape=(z_size,)),\n",
        "                          tf.keras.layers.Reshape(target_shape=(24, 24, 64)),\n",
        "                          tf.keras.layers.Conv2DTranspose(32, (3,3), activation='relu'),\n",
        "                          tf.keras.layers.Conv2DTranspose(1, (3,3)),\n",
        "                          ])\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoder_nn_output = self.encoder_nn(x)\n",
        "        z_mean, z_logvar = tf.split(encoder_nn_output, num_or_size_splits=2, axis=1)\n",
        "        return z_mean, z_logvar\n",
        "\n",
        "    def reparameterize(self, z_mean, z_logvar):\n",
        "        epsilon = tf.random.normal(shape=z_mean.shape)\n",
        "        z_sampled = epsilon * tf.exp(z_logvar * 0.5) + z_mean\n",
        "        return z_sampled\n",
        "      \n",
        "\n",
        "    def decode(self, z):\n",
        "        pixel_output = self.decoder_nn(z)\n",
        "        pixel_prob = tf.math.sigmoid(pixel_output)\n",
        "        return pixel_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ruRl4I4K0ea8"
      },
      "source": [
        "#### Define the loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y1QqvVSV0ea-",
        "colab": {}
      },
      "source": [
        "def calculate_ELBO(model, x):\n",
        "    z_mean, z_logvar = model.encode(x)\n",
        "    z = model.reparameterize(z_mean, z_logvar)\n",
        "    \n",
        "    pixel_prob = model.decode(z)\n",
        "    \n",
        "    logpx_z_pixels = tf.math.log(pixel_prob + 1e-10)*x + tf.math.log(1-pixel_prob + 1e-10)*(1-x)\n",
        "    logpx_z_images = tf.reduce_sum(logpx_z_pixels, axis=[1, 2, 3])\n",
        "    logpx_z = tf.reduce_mean (logpx_z_images)\n",
        "    \n",
        "    KL_parameters = -0.5 * (1 + z_logvar - (z_mean ** 2.0) - tf.exp(z_logvar))\n",
        "    KL_vectors = tf.reduce_sum(KL_parameters, axis=1)\n",
        "    KL = tf.reduce_mean(KL_vectors)\n",
        "    \n",
        "    ELBO = (logpx_z - KL)\n",
        "    \n",
        "    return -ELBO, logpx_z, KL #Negative because we want to maximise it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-uLxJpldXo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_pixel_squared(model, x):\n",
        "    z_mean, z_logvar = model.encode(x)\n",
        "    z = model.reparameterize(z_mean, z_logvar)\n",
        "    pixel_prob = model.decode(z)\n",
        "    pixel_squared = np.square(pixel_prob - x)\n",
        "    pixel_squared = tf.reduce_sum(pixel_squared, axis=[1,2,3])\n",
        "    pixel_squared = tf.reduce_mean (pixel_squared)\n",
        "    return pixel_squared"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toxPmTwwy0TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_ssim(model, x):\n",
        "  z_mean, z_logvar = model.encode(x)\n",
        "  z = model.reparameterize(z_mean, z_logvar)\n",
        "  y = model.decode(z)\n",
        "\n",
        "  x = tf.convert_to_tensor(x)\n",
        "  y = tf.convert_to_tensor(y)\n",
        "  ssim = tf.image.ssim(x[:,:,:,:],y[:,:,:,:], input_range, filter_size=11,filter_sigma=0.1,k1=0.1,k2=0.01)\n",
        "  ssim = tf.reduce_mean(ssim)\n",
        "  return (1-ssim)*1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vLIG3YRm0ebE"
      },
      "source": [
        "#### Define the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n5eaPpso0ebG",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NvoBcADVjvek"
      },
      "source": [
        "#### Define loss metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhYcUUvYjven",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VngH-amb0ebM"
      },
      "source": [
        "#### Define train and test steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dNN76Ed_0ebO",
        "colab": {}
      },
      "source": [
        "def train_step(model, x, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss,_,_ = calculate_ELBO(model, x)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        train_loss(loss)\n",
        "\n",
        "def test_step(model, x, optimizer):\n",
        "    loss,_,_ = calculate_ELBO(model, x)\n",
        "    test_loss(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6eTvDNOZGjW",
        "colab_type": "text"
      },
      "source": [
        "#### Generate random image from latent vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JUG20q7J0ebW",
        "colab": {}
      },
      "source": [
        "def generate_images(model, z_random, figures_to_plot):\n",
        "    generated_prob = model.decode(z_random)\n",
        "    generated_prob = np.squeeze(generated_prob, axis=3)\n",
        "    plot_images(generated_prob, _, figures_to_plot, include_labels=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SprYQb15_4ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_dataset(model, train_dataset, test_dataset, epochs, generate=True):\n",
        "  plots_per_epoch = 10\n",
        "  z_random = tf.random.normal(shape=[plots_per_epoch, z_size])\n",
        "  \n",
        "  test_ELBO = []\n",
        "  train_ELBO = []\n",
        "  epoch_number = []  \n",
        "  for epoch in range(epochs):\n",
        "    print('Epoch {}'.format(epoch))\n",
        "    for train_x in train_dataset: \n",
        "      train_step(model, train_x, optimizer)\n",
        "  \n",
        "    for test_x in test_dataset:\n",
        "      test_step(model, test_x, optimizer)\n",
        "    \n",
        "    test_ELBO.append(-test_loss.result())\n",
        "    train_ELBO.append(-train_loss.result())\n",
        "    epoch_number.append(epoch)\n",
        "    \n",
        "    clear_output()\n",
        "    if generate:\n",
        "      generate_images(model, z_random, [0,plots_per_epoch])\n",
        "    plt.plot(epoch_number, test_ELBO, train_ELBO)\n",
        "    plt.legend(['test','train'])\n",
        "    plt.title('model')\n",
        "    plt.show()\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b4MLsF1E0ebZ"
      },
      "source": [
        "#### Train the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_uvcaHy-Zcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def change_ratio(x, y, chosen_classes, chosen_numbers):\n",
        "    x_new = np.zeros([0,image_shape[0], image_shape[1]])\n",
        "    y_new = np.zeros([0])\n",
        "    n_classes = len(chosen_classes)\n",
        "    for i in range (n_classes):\n",
        "        x_class, y_class = pick_class(x, y, [chosen_classes[i]])\n",
        "        x_class = cut_data(x_class, chosen_numbers[i])\n",
        "        y_class = cut_data(y_class, chosen_numbers[i])\n",
        "        x_new = np.concatenate((x_new, x_class))\n",
        "        y_new = np.concatenate((y_new, y_class))\n",
        "    return x_new, y_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_cbixyTqI40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = ['Zero','One', 'Two', 'Three', 'Four', 'Five','Six', 'Seven', 'Eight', 'Nine']\n",
        "N_image_channels = 1\n",
        "N_class = len(class_names)\n",
        "image_shape = x_train.shape[1:3]\n",
        "input_range = np.amax(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pi8CDE0g0eba",
        "colab": {}
      },
      "source": [
        "chosen_classes=[0,1,2,3,4,5,6,7,8,9]\n",
        "chosen_numbers = [5, 10, 50, 100, 500, 1000, 2000, 3000, 4500, 6000]\n",
        "x_train, y_train = change_ratio(x_train, y_train, chosen_classes, chosen_numbers)\n",
        "chosen_numbers = [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
        "x_test, y_test = change_ratio(x_test, y_test, chosen_classes, chosen_numbers)\n",
        "\n",
        "x_train_0, y_train_0, x_test_0, y_test_0, train_dataset_0, test_dataset_0 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[0])\n",
        "x_train_1, y_train_1, x_test_1, y_test_1, train_dataset_1, test_dataset_1 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[1])\n",
        "x_train_2, y_train_2, x_test_2, y_test_2, train_dataset_2, test_dataset_2 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[2])\n",
        "x_train_3, y_train_3, x_test_3, y_test_3, train_dataset_3, test_dataset_3 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[3])\n",
        "x_train_4, y_train_4, x_test_4, y_test_4, train_dataset_4, test_dataset_4 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[4])\n",
        "x_train_5, y_train_5, x_test_5, y_test_5, train_dataset_5, test_dataset_5 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[5])\n",
        "x_train_6, y_train_6, x_test_6, y_test_6, train_dataset_6, test_dataset_6 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[6])\n",
        "x_train_7, y_train_7, x_test_7, y_test_7, train_dataset_7, test_dataset_7 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[7])\n",
        "x_train_8, y_train_8, x_test_8, y_test_8, train_dataset_8, test_dataset_8 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[8])\n",
        "x_train_9, y_train_9, x_test_9, y_test_9, train_dataset_9, test_dataset_9 = setup_dataset(x_train, y_train, x_test, y_test, chosen_classes=[9])\n",
        "\n",
        "z_size = 25\n",
        "model_0 = CVAE(z_size)\n",
        "model_1 = CVAE(z_size)\n",
        "model_2 = CVAE(z_size)\n",
        "model_3 = CVAE(z_size)\n",
        "model_4 = CVAE(z_size)\n",
        "model_5 = CVAE(z_size)\n",
        "model_6 = CVAE(z_size)\n",
        "model_7 = CVAE(z_size)\n",
        "model_8 = CVAE(z_size)\n",
        "model_9 = CVAE(z_size)\n",
        "\n",
        "model_dict = {'0' : model_0,\n",
        "              '1' : model_1,\n",
        "\t\t          '2' : model_2,\n",
        "              '3' : model_3,\n",
        "              '4' : model_4,\n",
        "              '5' : model_5,\n",
        "              '6' : model_6,\n",
        "              '7' : model_7,\n",
        "              '8' : model_8,\n",
        "              '9' : model_9,}\n",
        "\n",
        "train_test_dataset(model_0, train_dataset_0, test_dataset_0, 50, generate=True)\n",
        "train_test_dataset(model_1, train_dataset_1, test_dataset_1, 40, generate=True)\n",
        "train_test_dataset(model_2, train_dataset_2, test_dataset_2, 30, generate=True)\n",
        "train_test_dataset(model_3, train_dataset_3, test_dataset_3, 25, generate=True)\n",
        "train_test_dataset(model_4, train_dataset_4, test_dataset_4, 25, generate=True)\n",
        "train_test_dataset(model_5, train_dataset_5, test_dataset_5, 25, generate=True)\n",
        "train_test_dataset(model_6, train_dataset_6, test_dataset_6, 25, generate=True)\n",
        "train_test_dataset(model_7, train_dataset_7, test_dataset_7, 25, generate=True)\n",
        "train_test_dataset(model_8, train_dataset_8, test_dataset_8, 25, generate=True)\n",
        "train_test_dataset(model_9, train_dataset_9, test_dataset_9, 25, generate=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnr-d-OQZCI5",
        "colab_type": "text"
      },
      "source": [
        "#### Reconstruct images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jz0G3WmQD2vI",
        "colab": {}
      },
      "source": [
        "def reconstruct_images(model, images):\n",
        "    images_n = len(images)\n",
        "    x = images/input_range\n",
        "    #x[x >= 0.5] = 1.0\n",
        "    #x[x < 0.5] = 0.0\n",
        "    x = x.reshape(images_n, image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
        "    \n",
        "    z_mean, z_logvar = model.encode(x)\n",
        "    z = model.reparameterize(z_mean, z_logvar)\n",
        "    pixel_output = model.decode(z)\n",
        "    pixel_prob = tf.math.sigmoid(pixel_output)\n",
        "    pixel_prob = np.squeeze(pixel_prob, axis=3)\n",
        "\n",
        "    loss = np.empty([images_n])\n",
        "    for i in range(images_n):\n",
        "        loss[i],_,_ = calculate_ELBO(model,x[i:i+1])\n",
        "    loss = loss.astype('float16')\n",
        "    \n",
        "    print('Original Pictures:')\n",
        "    plot_images(images, _, figures_to_plot=[0,images_n], include_labels=False)\n",
        "    print('Reconstructed Pictures, with ELBO loss:')\n",
        "    plot_images(pixel_prob, -loss, figures_to_plot=[0,images_n], include_labels=True)\n",
        "\n",
        "reconstruct_images(model_0, x_test_0[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8juDbid_YrGS",
        "colab_type": "text"
      },
      "source": [
        "#### Visualize the latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nOgW1CZIjvfG",
        "colab": {}
      },
      "source": [
        "def visualize_latent_space(model, range1, range2, data_x, data_y, a, b):\n",
        "  plot = plt.figure(figsize=[7,7])\n",
        "  outliers=0\n",
        "  for i in range(range1, range2):\n",
        "    pic_visualize = data_x[i]\n",
        "    pic_visualize = pic_visualize.reshape(1, image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
        "    \n",
        "    z_mean, z_logvar = model.encode(pic_visualize)\n",
        "    z = model.reparameterize(z_mean, z_logvar)\n",
        "    \n",
        "    if any(z_logvar[:,i]>5 for i in range(z_size)):\n",
        "      outliers +=1\n",
        "      continue\n",
        "      \n",
        "    if data_y[i] == 0:\n",
        "      color = 'blue'\n",
        "    if data_y[i] == 1:\n",
        "      color = 'orange'\n",
        "    if data_y[i] == 2:\n",
        "      color = 'green'\n",
        "    if data_y[i] == 3:\n",
        "      color = 'red'\n",
        "    if data_y[i] == 4:\n",
        "      color = 'purple'  \n",
        "    if data_y[i] == 5:\n",
        "      color = 'brown'\n",
        "    if data_y[i] == 6:\n",
        "      color = 'pink'\n",
        "    if data_y[i] == 7:\n",
        "      color = 'gray'\n",
        "    if data_y[i] == 8:\n",
        "      color = 'black'\n",
        "    if data_y[i] == 9:\n",
        "      color = 'yellow'\n",
        "    plt.scatter(z[:,a],z[:,b], color=color, s=10)\n",
        "    #plt.xlim([-1000,1000])\n",
        "    #plt.ylim([-1000,1000])\n",
        "    #plt.annotate('{}'.format(i),(z[:,a],z[:,b]))\n",
        "  print('{} Outliers detected'.format(outliers))\n",
        "\n",
        "\n",
        "visualize_latent_space(model_8, 0,800, x_test_8, y_test_8, 0,1)\n",
        "\n",
        "#labels = []; \n",
        "#for i in range(N_train): labels.append(i)\n",
        "#plot_images(x_train, labels, [250,300])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohcKBv7HYyaS",
        "colab_type": "text"
      },
      "source": [
        "#### Check reconstruction performance for untrained classes and create table of losses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOUcblcZ4Djo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_of_losses = np.zeros([N_class, N_class])\n",
        "for i in range(0,N_class):\n",
        "  model = model_dict[str(i)]\n",
        "  print('Model trained with class {}'.format(i))\n",
        "  plt.figure(figsize=[9,4])\n",
        "  for j in range(N_class):\n",
        "    test_images = x_test[(y_test==j)]\n",
        "    test_images = set_pixels_binary(test_images)\n",
        "    test_images = test_images.reshape(len(test_images), image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
        "    #_,loss,_ = calculate_ELBO(model,test_images)\n",
        "    #loss = -calculate_pixel_squared(model,test_images)\n",
        "    loss = calculate_ssim(model, test_images)\n",
        "    table_of_losses[j,i] = loss\n",
        "    if j==i:\n",
        "      color='red'\n",
        "    else:\n",
        "      color='blue'\n",
        "    plt.bar(j,loss.numpy(),color=color)\n",
        "    #print('ELBO loss for class {}: {}'.format(i,loss))\n",
        "  plt.show()\n",
        "\n",
        "for i in range(N_class):\n",
        "    table_of_losses[i,:] = table_of_losses[i,:]/np.min(table_of_losses[i,:])\n",
        " \n",
        "index = list(str('-') * N_class)\n",
        "columns = list(str('-') * (N_class))\n",
        "for i in range(N_class):\n",
        "    index[i]= 'Class '+ str(i)\n",
        "    columns[i] = 'Model ' + str(i)\n",
        "\n",
        "table_loss = pd.DataFrame.from_records(np.around(table_of_losses, decimals=2), index=index, columns=columns)\n",
        "table_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO32aw-geBRj",
        "colab_type": "text"
      },
      "source": [
        "#### Classify test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbzrkU3FweFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chosen_numbers = [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
        "x_test_classes, y_test_classes = change_ratio(x_test, y_test, chosen_classes, chosen_numbers)\n",
        "\n",
        "predicted_class = np.zeros([1,len(x_test_classes)])\n",
        "predicted_probabilities = np.zeros([len(x_test_classes),10])\n",
        "for i in range(len(x_test_classes)):\n",
        "  if not i%10:\n",
        "    clear_output()\n",
        "    print('Progress:{}%'.format(100*(i+1)/len(x_test_classes)))\n",
        "  test_image = x_test_classes[i]/input_range\n",
        "  test_image = test_image.reshape(1, image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
        "  \n",
        "  loss_per_model = np.empty([1,10])\n",
        "  a = np.empty([1,10])\n",
        "  for j in range(0,10):\n",
        "    model = model_dict[str(j)]\n",
        "    loss1,_,_ = calculate_ELBO(model,test_image)\n",
        "    loss_per_model[0,j]= -loss1\n",
        "  a[0,:] = loss_per_model[0,:]/np.min(loss_per_model[0,:])\n",
        "  a = np.multiply(a,-2)\n",
        "  e_x = np.exp(a)\n",
        "  predicted_probabilities[i,:] = (e_x[0,:] / e_x[0,:].sum())*100\n",
        "  max_index = np.argmax(predicted_probabilities[i,:])\n",
        "  predicted_class[0,i] = max_index%10\n",
        "print(predicted_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXQXpNK5zsnB",
        "colab_type": "text"
      },
      "source": [
        "### Calculate losses for each VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeH1yZn3tdoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_losses(x):\n",
        "  losses_per_image = np.zeros([len(x),10])\n",
        "  for i in range(len(x)):\n",
        "    if not i%10:\n",
        "      clear_output()\n",
        "      print('Progress:{}%'.format(100*(i+1)/len(x)))\n",
        "    image = x[i]/input_range\n",
        "    image = image.reshape(1, image_shape[0], image_shape[1], N_image_channels).astype('float32')\n",
        "    for j in range(0,10):\n",
        "      model = model_dict[str(j)]\n",
        "      loss,_,_ = calculate_ELBO(model,image)\n",
        "      losses_per_image[i,j]= loss\n",
        "  return losses_per_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9lnVYIuCjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses_per_image_test = compute_losses(x_test_classes)\n",
        "losses_per_image_train = compute_losses(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZrCtjHgzwjX",
        "colab_type": "text"
      },
      "source": [
        "### Find classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyKb1zabz2kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def losses_softmax(x):\n",
        "  x_normalized = np.zeros(np.shape(x))\n",
        "  softmax_probabilities = np.zeros(np.shape(x))\n",
        "  for i in range(len(x)):\n",
        "    x_normalized[i,:] = x[i,:]/np.min(x[i,:])\n",
        "    x_normalized[i,:] = np.multiply(x_normalized[i,:],-2)\n",
        "    e_x = np.exp(x_normalized)\n",
        "    softmax_probabilities[i,:] = (np.exp(x_normalized[i,:]) / np.exp(x_normalized[i,:]).sum())*100\n",
        "  return softmax_probabilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmLbOWyz0OVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_probabilities_test = losses_softmax(losses_per_image_test)\n",
        "softmax_probabilities_train = losses_softmax(losses_per_image_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqs_wubP2dc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_highest_softmax(x):\n",
        "  predicted_class = np.zeros([len(x)],)\n",
        "  for i in range(len(x)):\n",
        "    predicted_class[i] = np.argmax(x[i,:])\n",
        "  return predicted_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOW0bRb227Me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_class_1 = classify_highest_softmax(softmax_probabilities_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEwFlXDV4n6v",
        "colab_type": "text"
      },
      "source": [
        "#### Create ROC curves and find optimal thresholds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPnysfBC3QSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ROC_thresholds(y, prob_x, plot_ROC_curves=1):\n",
        "  y_true = np.zeros([10,len(y)])\n",
        "  for i in range(len(y)):\n",
        "    y_true[y[i].astype('int'),i]=1 #each row has all the images, each row is a class. Values are 1 where the image corresponds to that class\n",
        "\n",
        "  y_score = np.zeros([10,len(y)])\n",
        "  for i in range(len(y)):\n",
        "    y_score[:,i] = prob_x[i] #each column is the softmax distribution for each image\n",
        "\n",
        "  \n",
        "  optimal_thresholds = np.zeros([10,])\n",
        "  for i in range(10):\n",
        "    fpr, tpr, threshold = roc_curve(y_true[i,:], y_score[i,:])\n",
        "    optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    optimal_thresholds[i] = optimal_threshold\n",
        "    if plot_ROC_curves == 1:\n",
        "      plot = plt.figure()\n",
        "      plt.plot(fpr, tpr, color='red')\n",
        "      plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "      plt.xlabel('False Positive Rate')\n",
        "      plt.ylabel('True Positive Rate')\n",
        "      plt.title('ROC curve for class '+str(i))\n",
        "      plt.show()\n",
        "  return optimal_thresholds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naTedHJJ709w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimal_thresholds = ROC_thresholds(y_test, softmax_probabilities_test, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI9P38cC6733",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_ROC_thresholds(prob_x, thresholds, y):\n",
        "  predicted_class = np.zeros([len(y),])\n",
        "  for i in range(len(y)):\n",
        "    diff = prob_x[i,:]-thresholds\n",
        "    predicted_class[i]=np.argmax(diff)\n",
        "  return predicted_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SyMAiq39GtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_class_2 = classify_ROC_thresholds(softmax_probabilities_test, optimal_thresholds, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8fLaFXBuzOs",
        "colab_type": "text"
      },
      "source": [
        "#### Check accuracy and plot misslabeled images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v37g0zTrabrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(predictions, y):\n",
        "  predictions = predictions.astype('int')\n",
        "  y = y.astype('int')\n",
        "\n",
        "  wrong_guesses = []\n",
        "  table_of_counts = np.zeros([N_class+1, N_class+1])\n",
        "  for i in range(len(predictions)):\n",
        "    table_of_counts[y[i], predictions[i]]+=1\n",
        "    if y[i] != predictions[i]:\n",
        "        wrong_guesses.append(i)\n",
        "  \n",
        "  for i in range(N_class):\n",
        "    accuracy_of_class = 100*table_of_counts[i,i]/np.sum(table_of_counts[i,0:-1])\n",
        "    table_of_counts[i,-1] = accuracy_of_class.astype('float16')\n",
        "  for i in range(N_class):\n",
        "    table_of_counts[-1,i] = np.sum(table_of_counts[0:10,i])-table_of_counts[i,i]\n",
        "\n",
        "  index = list(str('-') * (N_class+1))\n",
        "  columns = list(str('-') * (N_class+1))\n",
        "  for i in range(N_class):\n",
        "    index[i]= 'Class '+ str(i)\n",
        "    columns[i] = '|' + str(i) + '_Counts|'\n",
        "  index[-1] = 'False Positives'\n",
        "  columns[-1] = 'Percentage'\n",
        "  table = pd.DataFrame.from_records(table_of_counts, index=index, columns=columns)\n",
        "\n",
        "  accuracy = 100*table_of_counts.trace()/np.sum(table_of_counts[0:10,0:10])\n",
        "  average_accuracy = np.sum(table['Percentage'])/10\n",
        "\n",
        "  return table, accuracy, average_accuracy, wrong_guesses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV655SxYLN--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_1, accuracy_1, average_accuracy_1, wrong_guesses_1 = compute_accuracy(predicted_class_1, y_test)\n",
        "table_2, accuracy_2, average_accuracy_2, wrong_guesses_2 = compute_accuracy(predicted_class_2, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEbO5cLJZlfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracy_2)\n",
        "table_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV4d1Py2sXEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional VAE for each class results in accuracy of 98.16% with z_size = 10\n",
        "# Convolutional VAE for each class results in accuracy of 89.90% with z_size = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1qjNT_wICoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "table_same = pd.read_csv('/content/drive/My Drive/4YP/table_same.csv')\n",
        "table_increase = pd.read_csv('/content/drive/My Drive/4YP/table_increase.csv')\n",
        "table_decrease = pd.read_csv('/content/drive/My Drive/4YP/table_decrease.csv')\n",
        "table_loss_same = pd.read_csv('/content/drive/My Drive/4YP/table_loss_same.csv')\n",
        "table_loss_increase = pd.read_csv('/content/drive/My Drive/4YP/table_loss_increase.csv')\n",
        "table_loss_decrease = pd.read_csv('/content/drive/My Drive/4YP/table_loss_decrease.csv')\n",
        "predicted_prob_same = np.load('/content/drive/My Drive/4YP/predicted_prob_same.npy')\n",
        "predicted_prob_increase = np.load('/content/drive/My Drive/4YP/predicted_prob_increase.npy')\n",
        "predicted_prob_decrease = np.load('/content/drive/My Drive/4YP/predicted_prob_decrease.npy')\n",
        "losses_per_image_test = np.load('/content/drive/My Drive/4YP/losses_per_image_test.npy')\n",
        "losses_per_image_train = np.load('/content/drive/My Drive/4YP/losses_per_image_train.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T80ki3UympBu",
        "colab_type": "text"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TklN_hEIZTDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(128, activation= tf.nn.relu)\n",
        "    self.d2 = tf.keras.layers.Dense(128, activation= tf.nn.relu)\n",
        "    self.d3 = tf.keras.layers.Dense(10, activation = tf.nn.softmax)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.d3(x)\n",
        "    return x\n",
        "\n",
        "model = MyModel()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "#class_weights = [100, 80, 60, 40, 20, 10, 1, 0.1, 0.1, 0.1]\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "model.fit(losses_per_image_train, y_train, batch_size=50, epochs=20, class_weight=class_weights, validation_data=(losses_per_image_test, y_test), shuffle=True)\n",
        "model.evaluate(losses_per_image_test, y_test, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awOf8_dld5bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = np.argmax(model(losses_per_image_test), axis=1)\n",
        "table, _, average_accuracy, _ = compute_accuracy(results, y_test)\n",
        "print(average_accuracy)\n",
        "table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVVH0pLtOhr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8iJVeKvXZwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}